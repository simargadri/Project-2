{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Close</th>\n",
       "      <th>TSX Close</th>\n",
       "      <th>Bar Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>1117.699951</td>\n",
       "      <td>11866.900391</td>\n",
       "      <td>35.995537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1118.099976</td>\n",
       "      <td>11888.099609</td>\n",
       "      <td>36.337681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1135.900024</td>\n",
       "      <td>11944.500000</td>\n",
       "      <td>36.936466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1133.099976</td>\n",
       "      <td>11887.500000</td>\n",
       "      <td>36.474537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1138.199951</td>\n",
       "      <td>11953.799805</td>\n",
       "      <td>36.560101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-21</th>\n",
       "      <td>1879.199951</td>\n",
       "      <td>17500.900391</td>\n",
       "      <td>29.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-22</th>\n",
       "      <td>1866.599976</td>\n",
       "      <td>17552.500000</td>\n",
       "      <td>29.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>1874.699951</td>\n",
       "      <td>17593.599609</td>\n",
       "      <td>29.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>1879.699951</td>\n",
       "      <td>17543.400391</td>\n",
       "      <td>29.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>1891.000000</td>\n",
       "      <td>17545.800781</td>\n",
       "      <td>29.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Gold Close     TSX Close  Bar Close\n",
       "Date                                            \n",
       "2010-01-04  1117.699951  11866.900391  35.995537\n",
       "2010-01-05  1118.099976  11888.099609  36.337681\n",
       "2010-01-06  1135.900024  11944.500000  36.936466\n",
       "2010-01-07  1133.099976  11887.500000  36.474537\n",
       "2010-01-08  1138.199951  11953.799805  36.560101\n",
       "...                 ...           ...        ...\n",
       "2020-12-21  1879.199951  17500.900391  29.590000\n",
       "2020-12-22  1866.599976  17552.500000  29.040001\n",
       "2020-12-23  1874.699951  17593.599609  29.389999\n",
       "2020-12-29  1879.699951  17543.400391  29.360001\n",
       "2020-12-30  1891.000000  17545.800781  29.660000\n",
       "\n",
       "[2683 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gold.csv', index_col=\"Date\", infer_datetime_format=True, parse_dates=True)\n",
    "df = df.drop(columns=[\"Volume\", \"Open\", \"Oil Close\", \"CAD Close\", \"Low\", \"High\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df\n",
    "df_output = df[[\"Bar Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_input, df_output, test_size=0.3, random_state=42, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1878, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Close</th>\n",
       "      <th>TSX Close</th>\n",
       "      <th>Bar Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-08</th>\n",
       "      <td>1346.000000</td>\n",
       "      <td>14985.299805</td>\n",
       "      <td>20.796661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-11</th>\n",
       "      <td>1331.000000</td>\n",
       "      <td>15040.299805</td>\n",
       "      <td>20.305853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-12</th>\n",
       "      <td>1328.000000</td>\n",
       "      <td>15143.400391</td>\n",
       "      <td>20.450211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-13</th>\n",
       "      <td>1323.400024</td>\n",
       "      <td>15126.799805</td>\n",
       "      <td>20.046015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-14</th>\n",
       "      <td>1324.699951</td>\n",
       "      <td>15172.700195</td>\n",
       "      <td>20.103760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-21</th>\n",
       "      <td>1879.199951</td>\n",
       "      <td>17500.900391</td>\n",
       "      <td>29.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-22</th>\n",
       "      <td>1866.599976</td>\n",
       "      <td>17552.500000</td>\n",
       "      <td>29.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>1874.699951</td>\n",
       "      <td>17593.599609</td>\n",
       "      <td>29.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>1879.699951</td>\n",
       "      <td>17543.400391</td>\n",
       "      <td>29.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>1891.000000</td>\n",
       "      <td>17545.800781</td>\n",
       "      <td>29.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Gold Close     TSX Close  Bar Close\n",
       "Date                                            \n",
       "2017-09-08  1346.000000  14985.299805  20.796661\n",
       "2017-09-11  1331.000000  15040.299805  20.305853\n",
       "2017-09-12  1328.000000  15143.400391  20.450211\n",
       "2017-09-13  1323.400024  15126.799805  20.046015\n",
       "2017-09-14  1324.699951  15172.700195  20.103760\n",
       "...                 ...           ...        ...\n",
       "2020-12-21  1879.199951  17500.900391  29.590000\n",
       "2020-12-22  1866.599976  17552.500000  29.040001\n",
       "2020-12-23  1874.699951  17593.599609  29.389999\n",
       "2020-12-29  1879.699951  17543.400391  29.360001\n",
       "2020-12-30  1891.000000  17545.800781  29.660000\n",
       "\n",
       "[805 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaler = MinMaxScaler()\n",
    "x_test_scaler = MinMaxScaler()\n",
    "\n",
    "y_train_scaler = MinMaxScaler()\n",
    "y_test_scaler = MinMaxScaler()\n",
    "\n",
    "x_train_scaler.fit(x_train)\n",
    "x_test_scaler.fit(x_test)\n",
    "\n",
    "y_train_scaler.fit(y_train)\n",
    "y_test_scaler.fit(y_test)\n",
    "\n",
    "x_train = x_train_scaler.transform(x_train)\n",
    "x_test = x_test_scaler.transform(x_test)\n",
    "\n",
    "y_train = y_train_scaler.transform(y_train)\n",
    "y_test = y_test_scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1\n",
    "batch = 44\n",
    "features = 3\n",
    "\n",
    "train_generator = TimeseriesGenerator(x_train, y_train, length=length, sampling_rate=1, batch_size=batch)\n",
    "test_generator = TimeseriesGenerator(x_test, y_test, length=length, sampling_rate=1, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "units = 64\n",
    "drop = 0.2\n",
    "\n",
    "#1st layer\n",
    "model.add(LSTM(units=units, return_sequences=True, input_shape=(length, features)))\n",
    "model.add(Dropout(drop))\n",
    "\n",
    "#2nd layer\n",
    "model.add(LSTM(units=units, return_sequences=True))\n",
    "model.add(Dropout(drop))\n",
    "\n",
    "#3rd layer\n",
    "model.add(LSTM(units=units))\n",
    "model.add(Dropout(drop))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             17408     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 64)             33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 83,521\n",
      "Trainable params: 83,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(ThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        val_loss = logs[\"val_loss\"]\n",
    "        if val_loss < self.threshold:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.2230 - val_loss: 0.0600\n",
      "Epoch 2/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0798 - val_loss: 0.0680\n",
      "Epoch 3/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0975 - val_loss: 0.0353\n",
      "Epoch 4/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0523 - val_loss: 0.0283\n",
      "Epoch 5/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0354 - val_loss: 0.0203\n",
      "Epoch 6/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0220 - val_loss: 0.0204\n",
      "Epoch 7/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 8/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0149 - val_loss: 0.0165\n",
      "Epoch 9/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0118 - val_loss: 0.0144\n",
      "Epoch 10/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0091 - val_loss: 0.0112\n",
      "Epoch 11/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 12/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0053 - val_loss: 0.0031\n",
      "Epoch 13/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0055 - val_loss: 0.0016\n",
      "Epoch 14/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0056 - val_loss: 0.0016\n",
      "Epoch 15/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0051 - val_loss: 0.0013\n",
      "Epoch 16/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0043 - val_loss: 9.7736e-04\n",
      "Epoch 17/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0046 - val_loss: 0.0026\n",
      "Epoch 18/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 19/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0060 - val_loss: 0.0071\n",
      "Epoch 20/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 21/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0046 - val_loss: 0.0022\n",
      "Epoch 22/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 23/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0048 - val_loss: 0.0031\n",
      "Epoch 24/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 25/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0039 - val_loss: 0.0021\n",
      "Epoch 26/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0038 - val_loss: 8.4102e-04\n",
      "Epoch 27/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 28/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0069 - val_loss: 0.0107\n",
      "Epoch 29/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0086 - val_loss: 0.0120\n",
      "Epoch 30/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0070 - val_loss: 0.0083\n",
      "Epoch 31/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0050 - val_loss: 0.0032\n",
      "Epoch 32/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0048 - val_loss: 0.0027\n",
      "Epoch 33/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 34/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 35/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0067 - val_loss: 0.0050\n",
      "Epoch 36/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 37/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0040 - val_loss: 0.0073\n",
      "Epoch 38/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0088 - val_loss: 0.0159\n",
      "Epoch 39/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0115 - val_loss: 0.0118\n",
      "Epoch 40/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0066 - val_loss: 0.0045\n",
      "Epoch 41/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0037 - val_loss: 0.0020\n",
      "Epoch 42/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0050 - val_loss: 0.0053\n",
      "Epoch 43/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 44/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0078 - val_loss: 0.0043\n",
      "Epoch 45/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 46/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 47/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0060 - val_loss: 0.0094\n",
      "Epoch 48/1000\n",
      "43/43 [==============================] - 3s 68ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 49/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0049 - val_loss: 0.0030\n",
      "Epoch 50/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 51/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 52/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0061 - val_loss: 0.0048\n",
      "Epoch 53/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0055 - val_loss: 0.0031\n",
      "Epoch 54/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0029 - val_loss: 8.9460e-04\n",
      "Epoch 55/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 56/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 57/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0057 - val_loss: 0.0047\n",
      "Epoch 58/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 59/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 60/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 61/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0057 - val_loss: 0.0041\n",
      "Epoch 62/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 63/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 64/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0039 - val_loss: 0.0072\n",
      "Epoch 65/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 66/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0048 - val_loss: 0.0032\n",
      "Epoch 67/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0023 - val_loss: 9.0824e-04\n",
      "Epoch 68/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 69/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 70/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0060 - val_loss: 0.0038\n",
      "Epoch 71/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 72/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0030 - val_loss: 0.0052\n",
      "Epoch 73/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0062 - val_loss: 0.0098\n",
      "Epoch 74/1000\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0068 - val_loss: 0.0053\n",
      "Epoch 75/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 76/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 77/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 78/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0057 - val_loss: 0.0046\n",
      "Epoch 79/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0040 - val_loss: 0.0016\n",
      "Epoch 80/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 81/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0043 - val_loss: 0.0083\n",
      "Epoch 82/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 83/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0042 - val_loss: 0.0022\n",
      "Epoch 84/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 85/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 86/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0056 - val_loss: 0.0058\n",
      "Epoch 87/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0045 - val_loss: 0.0019\n",
      "Epoch 88/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 89/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0042 - val_loss: 0.0075\n",
      "Epoch 90/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 91/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0041 - val_loss: 0.0019\n",
      "Epoch 92/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0019 - val_loss: 8.7178e-04\n",
      "Epoch 93/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 94/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 95/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 96/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0024 - val_loss: 9.5522e-04\n",
      "Epoch 97/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0032 - val_loss: 0.0051\n",
      "Epoch 98/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0057 - val_loss: 0.0066\n",
      "Epoch 99/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0049 - val_loss: 0.0030\n",
      "Epoch 100/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0023 - val_loss: 7.4543e-04\n",
      "Epoch 101/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 102/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 103/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0044 - val_loss: 0.0031\n",
      "Epoch 104/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0024 - val_loss: 8.7560e-04\n",
      "Epoch 105/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 106/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0050 - val_loss: 0.0056\n",
      "Epoch 107/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 108/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0022 - val_loss: 7.7493e-04\n",
      "Epoch 109/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 110/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 111/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0044 - val_loss: 0.0036\n",
      "Epoch 112/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0027 - val_loss: 8.5851e-04\n",
      "Epoch 113/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0026 - val_loss: 0.0045\n",
      "Epoch 114/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0057 - val_loss: 0.0066\n",
      "Epoch 115/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0053 - val_loss: 0.0030\n",
      "Epoch 116/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0024 - val_loss: 7.1678e-04\n",
      "Epoch 117/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 118/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0037 - val_loss: 0.0061\n",
      "Epoch 119/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0042 - val_loss: 0.0026\n",
      "Epoch 120/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0024 - val_loss: 9.5964e-04\n",
      "Epoch 121/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0026 - val_loss: 0.0042\n",
      "Epoch 122/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0048 - val_loss: 0.0051\n",
      "Epoch 123/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0041 - val_loss: 0.0019\n",
      "Epoch 124/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0019 - val_loss: 6.5380e-04\n",
      "Epoch 125/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 126/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 127/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 128/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0021 - val_loss: 8.8668e-04\n",
      "Epoch 129/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 130/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 131/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 132/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0021 - val_loss: 6.3876e-04\n",
      "Epoch 133/1000\n",
      "43/43 [==============================] - 4s 101ms/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 134/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0040 - val_loss: 0.0080\n",
      "Epoch 135/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 136/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0025 - val_loss: 9.1109e-04\n",
      "Epoch 137/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 138/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 139/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0054 - val_loss: 0.0025\n",
      "Epoch 140/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0021 - val_loss: 6.4719e-04\n",
      "Epoch 141/1000\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 142/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0037 - val_loss: 0.0061\n",
      "Epoch 143/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 144/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 145/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.0026 - val_loss: 0.0040\n",
      "Epoch 146/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 147/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 148/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 149/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0023 - val_loss: 0.0045\n",
      "Epoch 150/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 151/1000\n",
      "43/43 [==============================] - 4s 85ms/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 152/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 153/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 154/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 155/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0028 - val_loss: 9.6949e-04\n",
      "Epoch 156/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 157/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0027 - val_loss: 0.0070\n",
      "Epoch 158/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0041 - val_loss: 0.0064\n",
      "Epoch 159/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0029 - val_loss: 9.2343e-04\n",
      "Epoch 160/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 161/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 162/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 163/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0021 - val_loss: 5.9129e-04\n",
      "Epoch 164/1000\n",
      "43/43 [==============================] - 3s 68ms/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 165/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0030 - val_loss: 0.0060\n",
      "Epoch 166/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 167/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0020 - val_loss: 7.5097e-04\n",
      "Epoch 168/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 169/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 170/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 171/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0020 - val_loss: 5.8494e-04\n",
      "Epoch 172/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0018 - val_loss: 0.0037\n",
      "Epoch 173/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0036 - val_loss: 0.0071\n",
      "Epoch 174/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 175/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0021 - val_loss: 8.6381e-04\n",
      "Epoch 176/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 177/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 178/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 179/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0017 - val_loss: 7.0392e-04\n",
      "Epoch 180/1000\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 181/1000\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 182/1000\n",
      "43/43 [==============================] - 4s 85ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 183/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 184/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0024 - val_loss: 0.0035\n",
      "Epoch 185/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 186/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0023 - val_loss: 6.2798e-04\n",
      "Epoch 187/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 188/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0024 - val_loss: 0.0052\n",
      "Epoch 189/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 190/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0021 - val_loss: 8.3421e-04\n",
      "Epoch 191/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 192/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 193/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0033 - val_loss: 0.0020\n",
      "Epoch 194/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0020 - val_loss: 5.8487e-04\n",
      "Epoch 195/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 196/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0030 - val_loss: 0.0071\n",
      "Epoch 197/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 198/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0018 - val_loss: 8.8932e-04\n",
      "Epoch 199/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0022 - val_loss: 0.0036\n",
      "Epoch 200/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 201/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 202/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0016 - val_loss: 7.5434e-04\n",
      "Epoch 203/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0017 - val_loss: 0.0036\n",
      "Epoch 204/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0029 - val_loss: 0.0052\n",
      "Epoch 205/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 206/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 207/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0022 - val_loss: 0.0031\n",
      "Epoch 208/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 209/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0024 - val_loss: 7.7737e-04\n",
      "Epoch 210/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 211/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0025 - val_loss: 0.0045\n",
      "Epoch 212/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0029 - val_loss: 0.0059\n",
      "Epoch 213/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 214/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 215/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 216/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0037 - val_loss: 0.0023\n",
      "Epoch 217/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0021 - val_loss: 6.6760e-04\n",
      "Epoch 218/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0017 - val_loss: 0.0035\n",
      "Epoch 219/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0028 - val_loss: 0.0065\n",
      "Epoch 220/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 221/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 222/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 223/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 224/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0023 - val_loss: 7.0356e-04\n",
      "Epoch 225/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 226/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0021 - val_loss: 0.0042\n",
      "Epoch 227/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 228/1000\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0017 - val_loss: 8.0106e-04\n",
      "Epoch 229/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 230/1000\n",
      "43/43 [==============================] - 4s 89ms/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 231/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0027 - val_loss: 0.0010\n",
      "Epoch 232/1000\n",
      "43/43 [==============================] - 4s 94ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 233/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 234/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 235/1000\n",
      "43/43 [==============================] - 6s 145ms/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 236/1000\n",
      "43/43 [==============================] - 5s 106ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 237/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 238/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 239/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0020 - val_loss: 6.9784e-04\n",
      "Epoch 240/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 241/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0025 - val_loss: 0.0068\n",
      "Epoch 242/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 243/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0018 - val_loss: 8.8505e-04\n",
      "Epoch 244/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 245/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 246/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 247/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 248/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0024 - val_loss: 0.0047\n",
      "Epoch 249/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 250/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0019 - val_loss: 8.8763e-04\n",
      "Epoch 251/1000\n",
      "43/43 [==============================] - 2s 42ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 252/1000\n",
      "43/43 [==============================] - 2s 40ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 253/1000\n",
      "43/43 [==============================] - 2s 41ms/step - loss: 0.0024 - val_loss: 8.1644e-04\n",
      "Epoch 254/1000\n",
      "43/43 [==============================] - 2s 40ms/step - loss: 0.0015 - val_loss: 6.7790e-04\n",
      "Epoch 255/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 256/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 257/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 258/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0013 - val_loss: 8.7261e-04\n",
      "Epoch 259/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 260/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 261/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0016 - val_loss: 6.4140e-04\n",
      "Epoch 262/1000\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 263/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 264/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 265/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0017 - val_loss: 9.4490e-04\n",
      "Epoch 266/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 267/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 268/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 269/1000\n",
      "43/43 [==============================] - 3s 68ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 270/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0028 - val_loss: 0.0077\n",
      "Epoch 271/1000\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 0.0042 - val_loss: 0.0094\n",
      "Epoch 272/1000\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.0031 - val_loss: 0.0010\n",
      "Epoch 273/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 274/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0040 - val_loss: 0.0032\n",
      "Epoch 275/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 276/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0015 - val_loss: 9.7801e-04\n",
      "Epoch 277/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 278/1000\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 279/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0016 - val_loss: 9.8592e-04\n",
      "Epoch 280/1000\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 281/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 282/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0016 - val_loss: 5.9569e-04\n",
      "Epoch 283/1000\n",
      "43/43 [==============================] - 4s 94ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 284/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 285/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 286/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0014 - val_loss: 7.0100e-04\n",
      "Epoch 287/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 288/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 289/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 290/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0014 - val_loss: 7.2123e-04\n",
      "Epoch 291/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0015 - val_loss: 0.0039\n",
      "Epoch 292/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0026 - val_loss: 0.0057\n",
      "Epoch 293/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 294/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 295/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0025 - val_loss: 0.0044\n",
      "Epoch 296/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 297/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 298/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 299/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 300/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 301/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0020 - val_loss: 9.6192e-04\n",
      "Epoch 302/1000\n",
      "43/43 [==============================] - 3s 58ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 303/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0030 - val_loss: 0.0018\n",
      "Epoch 304/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0019 - val_loss: 7.2500e-04\n",
      "Epoch 305/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 306/1000\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 307/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 308/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0013 - val_loss: 7.9191e-04\n",
      "Epoch 309/1000\n",
      "43/43 [==============================] - 3s 78ms/step - loss: 0.0012 - val_loss: 7.7196e-04\n",
      "Epoch 310/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0014 - val_loss: 7.0410e-04\n",
      "Epoch 311/1000\n",
      "43/43 [==============================] - 5s 110ms/step - loss: 0.0014 - val_loss: 5.6844e-04\n",
      "Epoch 312/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0013 - val_loss: 6.9469e-04\n",
      "Epoch 313/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 314/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 315/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 316/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0012 - val_loss: 7.0953e-04\n",
      "Epoch 317/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 318/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 319/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0018 - val_loss: 8.4206e-04\n",
      "Epoch 320/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 321/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0028 - val_loss: 0.0104\n",
      "Epoch 322/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0037 - val_loss: 0.0071\n",
      "Epoch 323/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0022 - val_loss: 9.2677e-04\n",
      "Epoch 324/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0026 - val_loss: 0.0063\n",
      "Epoch 325/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0061 - val_loss: 0.0047\n",
      "Epoch 326/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0034 - val_loss: 8.8258e-04\n",
      "Epoch 327/1000\n",
      "43/43 [==============================] - 3s 77ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 328/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 329/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 330/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0014 - val_loss: 8.9276e-04\n",
      "Epoch 331/1000\n",
      "43/43 [==============================] - 4s 93ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 332/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 333/1000\n",
      "43/43 [==============================] - 3s 78ms/step - loss: 0.0014 - val_loss: 6.0429e-04\n",
      "Epoch 334/1000\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 0.0012 - val_loss: 9.4224e-04\n",
      "Epoch 335/1000\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 336/1000\n",
      "43/43 [==============================] - 6s 144ms/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 337/1000\n",
      "43/43 [==============================] - 4s 92ms/step - loss: 0.0012 - val_loss: 7.0373e-04\n",
      "Epoch 338/1000\n",
      "43/43 [==============================] - 4s 103ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 339/1000\n",
      "43/43 [==============================] - 4s 85ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 340/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0015 - val_loss: 5.8146e-04\n",
      "Epoch 341/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 342/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0021 - val_loss: 0.0064\n",
      "Epoch 343/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 344/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 345/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0022 - val_loss: 0.0044\n",
      "Epoch 346/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 347/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 348/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0014 - val_loss: 0.0034\n",
      "Epoch 349/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0027 - val_loss: 0.0047\n",
      "Epoch 350/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 351/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 352/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 353/1000\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.0018 - val_loss: 7.5412e-04\n",
      "Epoch 354/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 355/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 356/1000\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 357/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0014 - val_loss: 7.4236e-04\n",
      "Epoch 358/1000\n",
      "43/43 [==============================] - 4s 104ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 359/1000\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 360/1000\n",
      "43/43 [==============================] - 4s 85ms/step - loss: 0.0015 - val_loss: 5.6923e-04\n",
      "Epoch 361/1000\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 362/1000\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.0017 - val_loss: 0.0043\n",
      "Epoch 363/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 364/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0012 - val_loss: 8.1288e-04\n",
      "Epoch 365/1000\n",
      "43/43 [==============================] - 5s 125ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 366/1000\n",
      "43/43 [==============================] - 7s 166ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 367/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0019 - val_loss: 5.3050e-04\n",
      "Epoch 368/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 369/1000\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.0021 - val_loss: 0.0063\n",
      "Epoch 370/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 371/1000\n",
      "43/43 [==============================] - 4s 99ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 372/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0023 - val_loss: 0.0039\n",
      "Epoch 373/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 374/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0017 - val_loss: 6.7228e-04\n",
      "Epoch 375/1000\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 376/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0024 - val_loss: 0.0065\n",
      "Epoch 377/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 378/1000\n",
      "43/43 [==============================] - 3s 58ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 379/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 380/1000\n",
      "43/43 [==============================] - 4s 82ms/step - loss: 0.0025 - val_loss: 0.0012\n",
      "Epoch 381/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 382/1000\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 383/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 384/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0015 - val_loss: 7.5530e-04\n",
      "Epoch 385/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 386/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 387/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 6.7036e-04\n",
      "Epoch 388/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 389/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 390/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 391/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0011 - val_loss: 6.7298e-04\n",
      "Epoch 392/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0013 - val_loss: 9.2680e-04\n",
      "Epoch 393/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0014 - val_loss: 9.4058e-04\n",
      "Epoch 394/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0012 - val_loss: 5.4166e-04\n",
      "Epoch 395/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 396/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 397/1000\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 398/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0012 - val_loss: 8.3592e-04\n",
      "Epoch 399/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0016 - val_loss: 0.0037\n",
      "Epoch 400/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 401/1000\n",
      "43/43 [==============================] - 2s 44ms/step - loss: 0.0018 - val_loss: 6.1872e-04\n",
      "Epoch 402/1000\n",
      "43/43 [==============================] - 2s 45ms/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 403/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0029 - val_loss: 0.0074\n",
      "Epoch 404/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 405/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 406/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 407/1000\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.0025 - val_loss: 9.5348e-04\n",
      "Epoch 408/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 409/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 410/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 411/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0012 - val_loss: 6.0967e-04\n",
      "Epoch 412/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 413/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 414/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0013 - val_loss: 7.4768e-04\n",
      "Epoch 415/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 416/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 417/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 418/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 419/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 420/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 421/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0012 - val_loss: 8.7468e-04\n",
      "Epoch 422/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 423/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 424/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0016 - val_loss: 7.2790e-04\n",
      "Epoch 425/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 426/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 427/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 428/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 429/1000\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.0022 - val_loss: 0.0061\n",
      "Epoch 430/1000\n",
      "43/43 [==============================] - 4s 98ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 431/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 432/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 433/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 434/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 435/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0016 - val_loss: 0.0039\n",
      "Epoch 436/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 437/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0012 - val_loss: 8.7975e-04\n",
      "Epoch 438/1000\n",
      "43/43 [==============================] - 4s 101ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 439/1000\n",
      "43/43 [==============================] - 4s 95ms/step - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 440/1000\n",
      "43/43 [==============================] - 5s 125ms/step - loss: 0.0014 - val_loss: 5.5690e-04\n",
      "Epoch 441/1000\n",
      "43/43 [==============================] - 6s 135ms/step - loss: 0.0013 - val_loss: 0.0040\n",
      "Epoch 442/1000\n",
      "43/43 [==============================] - 5s 108ms/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 443/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0015 - val_loss: 8.8368e-04\n",
      "Epoch 444/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 445/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 446/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0014 - val_loss: 6.8155e-04\n",
      "Epoch 447/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 448/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 449/1000\n",
      "43/43 [==============================] - 4s 95ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 450/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 451/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 452/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0014 - val_loss: 6.5805e-04\n",
      "Epoch 453/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 454/1000\n",
      "43/43 [==============================] - 4s 99ms/step - loss: 0.0015 - val_loss: 0.0037\n",
      "Epoch 455/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0017 - val_loss: 0.0034\n",
      "Epoch 456/1000\n",
      "43/43 [==============================] - 4s 89ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 457/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 458/1000\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 459/1000\n",
      "43/43 [==============================] - 4s 93ms/step - loss: 0.0019 - val_loss: 6.2780e-04\n",
      "Epoch 460/1000\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.0015 - val_loss: 0.0039\n",
      "Epoch 461/1000\n",
      "43/43 [==============================] - 5s 108ms/step - loss: 0.0029 - val_loss: 0.0087\n",
      "Epoch 462/1000\n",
      "43/43 [==============================] - 6s 144ms/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 463/1000\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.0014 - val_loss: 0.0035\n",
      "Epoch 464/1000\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 465/1000\n",
      "43/43 [==============================] - 5s 110ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 466/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 467/1000\n",
      "43/43 [==============================] - 4s 101ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 468/1000\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.0017 - val_loss: 9.4580e-04\n",
      "Epoch 469/1000\n",
      "43/43 [==============================] - 3s 78ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 470/1000\n",
      "43/43 [==============================] - 6s 138ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 471/1000\n",
      "43/43 [==============================] - 5s 112ms/step - loss: 0.0017 - val_loss: 8.2223e-04\n",
      "Epoch 472/1000\n",
      "43/43 [==============================] - 4s 99ms/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 473/1000\n",
      "43/43 [==============================] - 4s 82ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 474/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 475/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0011 - val_loss: 9.7612e-04\n",
      "Epoch 476/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0010 - val_loss: 9.8282e-04\n",
      "Epoch 477/1000\n",
      "43/43 [==============================] - 5s 106ms/step - loss: 0.0014 - val_loss: 7.0504e-04\n",
      "Epoch 478/1000\n",
      "43/43 [==============================] - 4s 94ms/step - loss: 0.0012 - val_loss: 5.4433e-04\n",
      "Epoch 479/1000\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 480/1000\n",
      "43/43 [==============================] - 4s 97ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 481/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0012 - val_loss: 8.4602e-04\n",
      "Epoch 482/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0010 - val_loss: 9.1574e-04\n",
      "Epoch 483/1000\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 484/1000\n",
      "43/43 [==============================] - 9s 212ms/step - loss: 0.0014 - val_loss: 7.1449e-04\n",
      "Epoch 485/1000\n",
      "43/43 [==============================] - 9s 206ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 486/1000\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 487/1000\n",
      "43/43 [==============================] - 7s 169ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 488/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 489/1000\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 490/1000\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 491/1000\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.0021 - val_loss: 9.6556e-04\n",
      "Epoch 492/1000\n",
      "43/43 [==============================] - 5s 109ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 493/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0022 - val_loss: 0.0080\n",
      "Epoch 494/1000\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 495/1000\n",
      "43/43 [==============================] - 4s 101ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 496/1000\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 497/1000\n",
      "43/43 [==============================] - 4s 103ms/step - loss: 0.0035 - val_loss: 0.0017\n",
      "Epoch 498/1000\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 499/1000\n",
      "43/43 [==============================] - 4s 87ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 500/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 501/1000\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 0.0012 - val_loss: 6.2726e-04\n",
      "Epoch 502/1000\n",
      "43/43 [==============================] - 7s 159ms/step - loss: 0.0012 - val_loss: 9.1429e-04\n",
      "Epoch 503/1000\n",
      "43/43 [==============================] - 6s 144ms/step - loss: 0.0013 - val_loss: 5.3870e-04\n",
      "Epoch 504/1000\n",
      "43/43 [==============================] - 6s 136ms/step - loss: 0.0011 - val_loss: 6.8919e-04\n",
      "Epoch 505/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0011 - val_loss: 7.4201e-04\n",
      "Epoch 506/1000\n",
      "43/43 [==============================] - 4s 102ms/step - loss: 0.0010 - val_loss: 6.5393e-04\n",
      "Epoch 507/1000\n",
      "43/43 [==============================] - 4s 99ms/step - loss: 0.0010 - val_loss: 6.5013e-04\n",
      "Epoch 508/1000\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 9.9079e-04 - val_loss: 5.8197e-04\n",
      "Epoch 509/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0011 - val_loss: 5.8890e-04\n",
      "Epoch 510/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0010 - val_loss: 6.1843e-04\n",
      "Epoch 511/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 512/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0013 - val_loss: 0.0036\n",
      "Epoch 513/1000\n",
      "43/43 [==============================] - 4s 87ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 514/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 515/1000\n",
      "43/43 [==============================] - 4s 88ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 516/1000\n",
      "43/43 [==============================] - 4s 98ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 517/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0015 - val_loss: 7.3620e-04\n",
      "Epoch 518/1000\n",
      "43/43 [==============================] - 4s 84ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 519/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0021 - val_loss: 0.0077\n",
      "Epoch 520/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 521/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 522/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 523/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 524/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0015 - val_loss: 0.0011TA: 1s \n",
      "Epoch 525/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 526/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 527/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 528/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 529/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 9.3949e-04\n",
      "Epoch 530/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0011 - val_loss: 7.6522e-04\n",
      "Epoch 531/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 532/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0012 - val_loss: 6.9785e-04\n",
      "Epoch 533/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0010 - val_loss: 7.5195e-04\n",
      "Epoch 534/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 535/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0013 - val_loss: 6.0894e-04\n",
      "Epoch 536/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0011 - val_loss: 7.5223e-04\n",
      "Epoch 537/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 538/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0012 - val_loss: 8.3500e-04\n",
      "Epoch 539/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0010 - val_loss: 9.9619e-04\n",
      "Epoch 540/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0010 - val_loss: 7.7345e-04\n",
      "Epoch 541/1000\n",
      "43/43 [==============================] - 3s 68ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 542/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0014 - val_loss: 9.8296e-04\n",
      "Epoch 543/1000\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 0.0012 - val_loss: 6.9492e-04\n",
      "Epoch 544/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 545/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 546/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0015 - val_loss: 9.5462e-04\n",
      "Epoch 547/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 548/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0024 - val_loss: 0.0040\n",
      "Epoch 549/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 550/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 551/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0022 - val_loss: 0.0059\n",
      "Epoch 552/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 553/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 554/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 555/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0016 - val_loss: 5.3539e-04\n",
      "Epoch 556/1000\n",
      "43/43 [==============================] - 4s 102ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 557/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0012 - val_loss: 9.1530e-04\n",
      "Epoch 558/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 9.8653e-04 - val_loss: 6.5389e-04\n",
      "Epoch 559/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0011 - val_loss: 7.9068e-04\n",
      "Epoch 560/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0011 - val_loss: 6.1757e-04\n",
      "Epoch 561/1000\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.0010 - val_loss: 7.2860e-04\n",
      "Epoch 562/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 563/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 564/1000\n",
      "43/43 [==============================] - 4s 88ms/step - loss: 9.8760e-04 - val_loss: 6.1789e-04\n",
      "Epoch 565/1000\n",
      "43/43 [==============================] - 4s 97ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 566/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0013 - val_loss: 9.6599e-04\n",
      "Epoch 567/1000\n",
      "43/43 [==============================] - 5s 107ms/step - loss: 0.0011 - val_loss: 6.5243e-04\n",
      "Epoch 568/1000\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 569/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 570/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 571/1000\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 572/1000\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0048\n",
      "Epoch 573/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 574/1000\n",
      "43/43 [==============================] - 6s 146ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 575/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0017 - val_loss: 0.0068\n",
      "Epoch 576/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 577/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 578/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 579/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 580/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 581/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 582/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0011 - val_loss: 6.2341e-04\n",
      "Epoch 583/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0010 - val_loss: 5.4557e-04\n",
      "Epoch 584/1000\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 9.7569e-04 - val_loss: 5.8260e-04\n",
      "Epoch 585/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0010 - val_loss: 5.3112e-04\n",
      "Epoch 586/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 9.7514e-04 - val_loss: 5.6760e-04\n",
      "Epoch 587/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 9.5369e-04 - val_loss: 0.0012\n",
      "Epoch 588/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0011 - val_loss: 6.8504e-04\n",
      "Epoch 589/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 9.6041e-04 - val_loss: 8.9037e-04\n",
      "Epoch 590/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 591/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0013 - val_loss: 7.2609e-04\n",
      "Epoch 592/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 593/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 594/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0011 - val_loss: 9.6589e-04\n",
      "Epoch 595/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 9.1753e-04 - val_loss: 0.0010\n",
      "Epoch 596/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 597/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 598/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 599/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 600/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 601/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 602/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 603/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0025 - val_loss: 0.0062\n",
      "Epoch 604/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 605/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 606/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0016 - val_loss: 0.0048\n",
      "Epoch 607/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0023 - val_loss: 9.3449e-04\n",
      "Epoch 608/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 609/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 610/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 611/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 612/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 613/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0015 - val_loss: 9.9619e-04\n",
      "Epoch 614/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0014 - val_loss: 7.1962e-04\n",
      "Epoch 615/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0012 - val_loss: 6.2602e-04\n",
      "Epoch 616/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0011 - val_loss: 6.8243e-04\n",
      "Epoch 617/1000\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 618/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 619/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 620/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 621/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 622/1000\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 623/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0012 - val_loss: 9.7784e-04\n",
      "Epoch 624/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 625/1000\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 626/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 627/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 628/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0018 - val_loss: 0.0036\n",
      "Epoch 629/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 630/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0012 - val_loss: 6.1218e-04\n",
      "Epoch 631/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 632/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 633/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0015 - val_loss: 5.5507e-04\n",
      "Epoch 634/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 635/1000\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 636/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 637/1000\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 638/1000\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 639/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0016 - val_loss: 7.3725e-04\n",
      "Epoch 640/1000\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 641/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.0016 - val_loss: 6.8558e-04\n",
      "Epoch 642/1000\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 0.0011 - val_loss: 6.3243e-04\n",
      "Epoch 643/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 644/1000\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 645/1000\n",
      "43/43 [==============================] - 4s 94ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 646/1000\n",
      "43/43 [==============================] - 5s 109ms/step - loss: 9.8823e-04 - val_loss: 0.0011\n",
      "Epoch 647/1000\n",
      "43/43 [==============================] - 5s 108ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 648/1000\n",
      "43/43 [==============================] - 5s 112ms/step - loss: 0.0014 - val_loss: 9.4210e-04\n",
      "Epoch 649/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0011 - val_loss: 9.7345e-04\n",
      "Epoch 650/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 651/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 652/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0010 - val_loss: 9.5881e-04\n",
      "Epoch 653/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 654/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 655/1000\n",
      "43/43 [==============================] - 3s 77ms/step - loss: 0.0013 - val_loss: 6.5994e-04\n",
      "Epoch 656/1000\n",
      "43/43 [==============================] - 4s 103ms/step - loss: 9.7179e-04 - val_loss: 0.0014\n",
      "Epoch 657/1000\n",
      "43/43 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 0.0040\n",
      "Epoch 658/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0018 - val_loss: 7.1149e-04\n",
      "Epoch 659/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 660/1000\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 661/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 662/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 663/1000\n",
      "43/43 [==============================] - 4s 89ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 664/1000\n",
      "43/43 [==============================] - 4s 93ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 665/1000\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 0.0013 - val_loss: 6.7213e-04\n",
      "Epoch 666/1000\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.0013 - val_loss: 9.1192e-04\n",
      "Epoch 667/1000\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.0013 - val_loss: 6.1504e-04\n",
      "Epoch 668/1000\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 0.0010 - val_loss: 5.9606e-04\n",
      "Epoch 669/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 670/1000\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 671/1000\n",
      "43/43 [==============================] - 4s 97ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 672/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 673/1000\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 674/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 675/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0011 - val_loss: 9.3299e-04\n",
      "Epoch 676/1000\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 677/1000\n",
      "43/43 [==============================] - 4s 87ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 678/1000\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.0011 - val_loss: 6.5484e-04\n",
      "Epoch 679/1000\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 680/1000\n",
      "43/43 [==============================] - 4s 91ms/step - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 681/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 682/1000\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 683/1000\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 684/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 685/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 686/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 687/1000\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 688/1000\n",
      "43/43 [==============================] - 5s 111ms/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 689/1000\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 690/1000\n",
      "43/43 [==============================] - 3s 78ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 691/1000\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 0.0010 - val_loss: 7.7771e-04\n",
      "Epoch 692/1000\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 693/1000\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.0013 - val_loss: 9.6201e-04\n",
      "Epoch 694/1000\n",
      "43/43 [==============================] - 3s 58ms/step - loss: 0.0012 - val_loss: 7.6399e-04\n",
      "Epoch 695/1000\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 696/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 697/1000\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.0010 - val_loss: 6.8457e-04\n",
      "Epoch 698/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 9.9438e-04 - val_loss: 9.1641e-04\n",
      "Epoch 699/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 700/1000\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.0011 - val_loss: 7.0338e-04\n",
      "Epoch 701/1000\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 9.4577e-04 - val_loss: 7.8404e-04\n",
      "Epoch 702/1000\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 703/1000\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 704/1000\n",
      "43/43 [==============================] - 5s 108ms/step - loss: 0.0012 - val_loss: 4.9685e-04\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
    "\n",
    "earlystop = ThresholdCallback(threshold=0.00053)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "\n",
    "fit = model.fit_generator(train_generator, epochs = 1000, validation_data=test_generator,\n",
    "                         shuffle=False, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004968530395815737"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_generator, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prices = y_test_scaler.inverse_transform(predictions)\n",
    "real_prices = y_test_scaler.inverse_transform(y_test[1:].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-11</th>\n",
       "      <td>20.305853</td>\n",
       "      <td>21.046423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-12</th>\n",
       "      <td>20.450211</td>\n",
       "      <td>20.538908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-13</th>\n",
       "      <td>20.046015</td>\n",
       "      <td>20.672626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-14</th>\n",
       "      <td>20.103760</td>\n",
       "      <td>20.262386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-15</th>\n",
       "      <td>20.276985</td>\n",
       "      <td>20.319033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-21</th>\n",
       "      <td>29.590000</td>\n",
       "      <td>29.727850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-22</th>\n",
       "      <td>29.040001</td>\n",
       "      <td>29.548941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>29.389999</td>\n",
       "      <td>28.932455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>29.360001</td>\n",
       "      <td>29.315140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>29.660000</td>\n",
       "      <td>29.291681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Real  Predicted\n",
       "Date                            \n",
       "2017-09-11  20.305853  21.046423\n",
       "2017-09-12  20.450211  20.538908\n",
       "2017-09-13  20.046015  20.672626\n",
       "2017-09-14  20.103760  20.262386\n",
       "2017-09-15  20.276985  20.319033\n",
       "...               ...        ...\n",
       "2020-12-21  29.590000  29.727850\n",
       "2020-12-22  29.040001  29.548941\n",
       "2020-12-23  29.389999  28.932455\n",
       "2020-12-29  29.360001  29.315140\n",
       "2020-12-30  29.660000  29.291681\n",
       "\n",
       "[804 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "}, index = df.index[-len(real_prices): ]) \n",
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Actual Vs. Predicted Prices'}, xlabel='Date'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stocks.plot(title=\"Actual Vs. Predicted Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.025810592803195878"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks[\"diff\"] = stocks[\"Real\"] - stocks[\"Predicted\"]\n",
    "stocks[\"diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "file_path = Path(\"bar_model_1.json\")\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
